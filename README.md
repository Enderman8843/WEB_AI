<img width="1919" height="677" alt="image" src="https://summer.hackclub.com/rails/active_storage/blobs/redirect/eyJfcmFpbHMiOnsiZGF0YSI6NTkzMjIsInB1ciI6ImJsb2JfaWQifX0=--37be2eddde2f9f15abf6a0d2b0482e4e83dac40d/%F0%9D%9D%80%20(1).png" />


# Lanthanum.AI (Web_AI)
 Lanthanum.AI is a project with which the user can access small LLM models through web which would be running via WEB GPU and this doesn't require any kind of AI , Everything runs in local and nothing goes out of users pc.

PS: Added Live Video captioning feature (aka VLM) it uses Appleâ„¢ FastVLM 0.5B Model to caption the images and give it respones , The respones can be customized too

 # Installation

 ```
git clone https://github.com/Enderman8843/WEB_AI
cd WEB_AI
npm run dev
  ```
 

 # Demo 
 https://lanthanum.vercel.app/?model=SmolLM2-360M-Instruct-q0f32-MLC


# Compabality Issues
Lanthanum.AI is best recommened to run on Chromium based browser i.e Chrome and Edge  >= 113 , If using other broswers check this guide also this dosent work if your pc dosent support DirectX12
https://caniuse.com/webgpu 

If you can't use webGPU models try clicking on USE CPU btn on error.

# LanthanumVLM

https://github.com/user-attachments/assets/4d115b91-6e6e-4eec-9043-afc7ec7c2316




 
